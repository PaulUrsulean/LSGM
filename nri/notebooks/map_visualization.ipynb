{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Weather Station Interactions on Map\n",
    "This notebook contains code to load a trained encoder, predict latent interactions (probably on test set) and finally visualize this graph on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotting library if not already done\n",
    "# !conda install basemap basemap-data-hires -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix error in case Basemap can't find epsg files, see https://github.com/matplotlib/basemap/issues/419\n",
    "# You might need to adjust the path, e.g. your proj4-folder could have a slightly different name\n",
    "os.environ['PROJ_LIB'] = \"/nfs/homedirs/grafberg/anaconda3/pkgs/proj4-5.2.0-he6710b0_1/share/proj/\"\n",
    "sys.path.append(\"..\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible paths:  \n",
    "\n",
    "    (\"smoo_cnn_rnn_ts_14_edg_2_dyn_t_prior/Sat_Jun_15_23_44_28_2019-9223\",)\n",
    "    \"smoo_cnn_rnn_ts_14_edg_2_dyn_t_spars_prior/Sat_Jun_15_23_42_57_2019-9223\",\n",
    "    \"smoo_cnn_rnn_ts_14_edg_4_dyn_t_spars_prior/Sat_Jun_15_23_43_03_2019-9223\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import utils\n",
    "\n",
    "#### Paths and configuration regarding encoder\n",
    "BASE_PATH = \"/nfs/students/summer-term-2019/project-4/logs/weather/nri/\" \\\n",
    "            \"smoo_cnn_rnn_ts_14_edg_2_dyn_t_spars_prior/Sat_Jun_15_23_42_57_2019-9223\"\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, \"models\")\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, \"config.json\")\n",
    "EPOCH = utils.find_latest_checkpoint(MODEL_BASE_PATH)\n",
    "config = json.load(open(CONFIG_PATH, \"rt\"))\n",
    "\n",
    "\n",
    "# Relevant information about data set\n",
    "dataset = config['data']['name']\n",
    "n_features = config['data'][dataset]['dims']\n",
    "n_atoms = config['data'][dataset]['atoms']\n",
    "n_edge_types = config['model']['n_edge_types']\n",
    "timesteps = config['data']['timesteps']\n",
    "\n",
    "\n",
    "# Set data location\n",
    "DATA_PATH = os.path.abspath(\"/nfs/students/summer-term-2019/project-4/paul/datasets/weather/100000_5_100_1_0_raw_new.pickle\") \n",
    "STATION_INFOS_PATH = os.path.abspath(\"/nfs/students/summer-term-2019/project-4/andreas/project-4/notebooks/stations_coordinates.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set and information about stations\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "    \n",
    "with open(STATION_INFOS_PATH, \"rb\") as f:\n",
    "    station_infos = pickle.load(f)\n",
    "\n",
    "data = data_dict['test_set'] # Contains test set and all time series\n",
    "configs = data_dict['configurations'] # Which stations represent individual sequences\n",
    "config_indices = data_dict['test_config_indices'] # How to lookup configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Latent Graphs with Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Encoder\n",
    "from src.model.utils import update_model_weights_from_path, gen_fully_connected\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from train import create_encoder\n",
    "import torch\n",
    "\n",
    "# Change to device('cuda:0') e.g. to run on GPU\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "# Load model as defined in config and \n",
    "encoder = create_encoder(config).to(device)\n",
    "encoder.load_state_dict(torch.load(os.path.join(MODEL_BASE_PATH, f\"encoder_epoch{EPOCH}.pt\"), map_location='cpu'))\n",
    "print(f\"Successfully loaded encoder from epoch {EPOCH}\")\n",
    "# Move data to GPU if used\n",
    "data_tensor = torch.Tensor(data).to(device)\n",
    "data_loader = DataLoader(TensorDataset(data_tensor), batch_size=128, shuffle=False) # TODO: Wrap in dataloader\n",
    "\n",
    "rel_rec, rel_send = gen_fully_connected(n_atoms, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test set through encoder and extract graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils import gumbel_softmax, edges_to_adj\n",
    "\n",
    "encoder.eval()\n",
    "graphs = np.empty((0, n_edge_types, n_atoms, n_atoms))\n",
    "\n",
    "for batch_id, (batch,) in tqdm(enumerate(data_loader)):\n",
    "    # Only use 'timesteps'- number of values to infer latent graph\n",
    "    batch = batch[:, :, :timesteps, :].to(device)\n",
    "\n",
    "    logits = encoder(batch, rel_rec, rel_send)\n",
    "    edges = gumbel_softmax(logits, tau=0.5, hard=True).cpu()\n",
    "    graphs_batch = edges_to_adj(edges, n_atoms=n_atoms)\n",
    "    graphs = np.concatenate((graphs, graphs_batch), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances are normalized between 0 and 1 to match\n",
    "# the latent graph activations\n",
    "with open(\"/nfs/students/summer-term-2019/project-4/paul/notebooks/station_distances.pickle\", \"rb\") as f:\n",
    "    station_distances = pickle.load(f)\n",
    "\n",
    "def compare_graph_with_distances(sample_id, rel_ix, corr=None):\n",
    "        \n",
    "    graph_to_compare = graphs[sample_id][rel_ix] if corr is None else corr\n",
    "    stations = configs[config_indices[sample_id]]\n",
    "    \n",
    "    loss = 0.0\n",
    "    \n",
    "    # Check the graph symetrically, omitting the diagonal\n",
    "    for i in range(n_atoms):\n",
    "        for j in range(i+1, n_atoms):\n",
    "            dist = station_distances[stations[i]][stations[j]]\n",
    "            \n",
    "            # Reverses the distances, i.e. makes longest distance 0 and shortest distance 1\n",
    "            dist = abs(dist-1)\n",
    "            \n",
    "            # Both symmetric entries over and under the diagonal are checked against the distance.\n",
    "            # This is done with the assumption that most relationships will be bi-directional\n",
    "            loss += abs(graph_to_compare[i][j] - dist)\n",
    "            loss += abs(graph_to_compare[j][i] - dist)\n",
    "            \n",
    "    # Return mean error\n",
    "    return loss/(n_atoms*(n_atoms-1))\n",
    "\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "corr_loss = []\n",
    "\n",
    "for rel in range(n_edge_types):\n",
    "    losses.append([])\n",
    "for g in range(len(graphs)):\n",
    "    corr = np.corrcoef(data[g, :, :, 0])\n",
    "    corr_loss.append(compare_graph_with_distances(g, rel, corr))\n",
    "    for rel in range(n_edge_types):\n",
    "        losses[rel].append(compare_graph_with_distances(g, rel))\n",
    "\n",
    "for loss_ix in range(len(losses)):\n",
    "    print(\"Rel {} latent graph error: {}\".format(loss_ix, sum(losses[loss_ix])/len(losses[loss_ix])))\n",
    "print(\"Correlation matrix error: {}\".format(sum(corr_loss)/len(corr_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the fun part - Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Map to plot on\n",
    "As our data set's weather stations are located in Spain, the below used coordinates represent a square fully comprising Spain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils import id_2_loc, edges_to_adj\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Map to plot weather stations\n",
    "map = Basemap(projection='lcc', resolution='l',\n",
    "            width=1.5E6, height=1.5E6,#width=3E6, height=3E6, #\n",
    "            lat_0=39, lon_0=-4.5) #lat_0=38, lon_0=-2.5)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather.utils import plot_interactions\n",
    "\n",
    "def plot_configuration(index, ax=None, skip_first=True):\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, figsize=(15, 15))\n",
    "    \n",
    "    configuration = configs[config_indices[sample_id]]\n",
    "    locations = [(station_infos[i]['long'], station_infos[i]['lat']) for i in configuration]\n",
    "\n",
    "    plot_interactions(locations, graphs[sample_id], ax=ax, map=map, skip_first=True)\n",
    "\n",
    "def get_sample_ids_with_configuration(config_id):\n",
    "    sample_ids = [i for i, val in enumerate(config_indices) if val == config_id]\n",
    "    return sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interactions on Map\n",
    "sample_id = 3\n",
    "plot_configuration(sample_id)\n",
    "#plt.savefig(f\"final_plots/2_sparse_id_{sample_id}_map.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adjacency matrix\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_edge_types, figsize=(20, 5))\n",
    "for i in range(n_edge_types):\n",
    "    im = axes[i].imshow(graphs[sample_id, i, :, :], cmap='gray', interpolation=None)\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "#plt.savefig(f\"final_plots/2_sparse_id_{sample_id}_adj.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average graph for a configuration:\n",
    "print(f\"There exists a total of {len(np.unique(config_indices))} different configurations in our test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average interactions for this configuration\n",
    "indices = get_sample_ids_with_configuration(config_indices[sample_id])\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_edge_types, figsize=(20, 5))\n",
    "for i in range(n_edge_types):\n",
    "    im = axes[i].imshow(graphs[indices, i, :, :].mean(axis=0), cmap='gray', interpolation=None)\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.7)\n",
    "#plt.savefig(f\"final_plots/2_sparse_id_{sample_id}_mean_adj.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average interactions across all samples\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_edge_types, figsize=(20, 5))\n",
    "for i in range(n_edge_types):\n",
    "    im = axes[i].imshow(graphs[:, i, :, :].mean(axis=0), cmap='gray', interpolation=None)\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simply show correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "corr = np.corrcoef(data[sample_id, :, :, 0])\n",
    "ax[0].set_title(\"Corr Matrix\")\n",
    "im = ax[0].imshow(corr, cmap='gray', interpolation=None)\n",
    "\n",
    "ax[1].set_title(\"Thresholded Matrix\")\n",
    "corr_thresh = corr > np.percentile(corr, 30)\n",
    "ax[1].imshow(corr_thresh, cmap='gray', interpolation=None)\n",
    "fig.colorbar(im, ax=ax.ravel().tolist(), shrink=0.7)\n",
    "#fig.savefig(f\"final_plots/corr_id_{sample_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the latent graph changes over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def latent_graph_dynamic(sample_id):\n",
    "    dynamic_graphs = []\n",
    "    sliding_windows = []\n",
    "    for step in range(0, 100 - timesteps): #data_tensor.size(-2) - timesteps):\n",
    "        # Use sliding window to update latent graph over timesteps\n",
    "        input = data_tensor[sample_id:sample_id+1, :, step:step+timesteps, :]\n",
    "        logits = encoder(input, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=0.5, hard=True).cpu()\n",
    "        graphs = edges_to_adj(edges, n_atoms=n_atoms)\n",
    "        \n",
    "        dynamic_graphs.append(graphs)\n",
    "        sliding_windows.append(input[0].cpu().detach().numpy())\n",
    "        \n",
    "    return np.asanyarray(sliding_windows), np.asanyarray(dynamic_graphs)[:,0,:,:,:]\n",
    "\n",
    "def init_anim(data, graph, axarr=None):\n",
    "    \n",
    "    # Create axis to plot if not given\n",
    "    n_edge_types = graph.shape[0]\n",
    "    if axarr is None:\n",
    "        fig, axarr = plt.subplots(nrows=1, ncols=1 + n_edge_types)\n",
    "    else:\n",
    "        assert(len(axarr) >= 1 + n_edge_types)\n",
    "    \n",
    "    # Plot data\n",
    "    n_atoms = data.shape[0]\n",
    "    data_plots = []\n",
    "    axarr[0].set_ylim((-2.0, 2.0))\n",
    "    axarr[0].set_title(\"Data (Sliding Window)\")\n",
    "    for atom in range(n_atoms):\n",
    "        data_plots.append(axarr[0].plot(data[atom,:,0], label=f\"Station {atom}\")[0])\n",
    "        \n",
    "    axarr[0].legend()\n",
    "        \n",
    "        \n",
    "    # Plot latent graphs\n",
    "    \n",
    "    graph_plots = []\n",
    "    for i in range(n_edge_types):\n",
    "        axarr[i + 1].set_title(f\"Edge type #{i}\")\n",
    "        im = axarr[i + 1].imshow(graphs[0, i, :, :], cmap='gray', interpolation=None)\n",
    "        graph_plots.append(im)\n",
    "    \n",
    "    return data_plots, graph_plots\n",
    "\n",
    "\n",
    "def update_anim(step):\n",
    "    global data_plots\n",
    "    global graph_plots\n",
    "    global sliding_windows\n",
    "    global data\n",
    "    global graphs\n",
    "    \n",
    "    \n",
    "    step = int(step)\n",
    "    n_atoms = data.shape[1]\n",
    "    timesteps = data.shape[2]\n",
    "    for atom in range(n_atoms):\n",
    "        sequence = sliding_windows[step, atom, :, 0]\n",
    "        new_data = (np.arange(len(sequence)), sequence)\n",
    "        data_plots[atom].set_data(new_data)\n",
    "        \n",
    "    n_edge_types = graphs.shape[1]\n",
    "    for i in range(n_edge_types):\n",
    "        graph_plots[i].set_data(graphs[step, i, :, :])\n",
    "    return (data_plots + graph_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(nrows=1, ncols=n_edge_types + 1, figsize=(20, 5))\n",
    "\n",
    "sliding_windows, graphs = latent_graph_dynamic(sample_id)\n",
    "data_plots, graph_plots = init_anim(sliding_windows[0], graphs[0], axarr=axarr)\n",
    "    \n",
    "ani = FuncAnimation(fig, update_anim, frames=len(graphs), interval=300, blit=True, repeat=True)\n",
    "ani.save(f\"final_plots/2_sparse_id_{sample_id}_anim.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mllab-venv)",
   "language": "python",
   "name": "mllab-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
